{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rich.progress import track\n",
    "\n",
    "import torch\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "from nlpaug.util import Action\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1，bert roberta  mask  aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_augment(data_source, data_target, textcol=\"text\", aug_p=0.2, device1=\"cuda\", device2=\"cuda\"):\n",
    "    ### contextual augmentation \n",
    "    print(f\"\\n-----transformer_augment-----\\n\")\n",
    "    augmenter1 = naw.ContextualWordEmbsAug(\n",
    "        model_path='roberta-base', action=\"substitute\", aug_min=1, aug_p=aug_p, device=device1)\n",
    "\n",
    "    augmenter2 = naw.ContextualWordEmbsAug(\n",
    "        model_path='bert-base-uncased', action=\"substitute\", aug_min=1, aug_p=aug_p, device=device2)\n",
    "\n",
    "    train_data = pd.read_csv(data_source)\n",
    "    train_text = train_data[textcol].fillna('.').astype(str).values\n",
    "    print(\"train_text:\", len(train_text), type(train_text[0]))\n",
    "\n",
    "    auglist1, auglist2 = [], []\n",
    "    for txt in train_text:\n",
    "        atxt1 = augmenter1.augment(txt)\n",
    "        atxt2 = augmenter2.augment(txt)\n",
    "        auglist1.append(atxt1[0])\n",
    "        auglist2.append(atxt2[0])\n",
    "\n",
    "    train_data['roberta_'+str(int(aug_p*100))] = pd.Series(auglist1)\n",
    "    train_data['bert_'+str(int(aug_p*100))] = pd.Series(auglist2)\n",
    "    train_data.to_csv(data_target, index=False)\n",
    "\n",
    "    for o, a1, a2 in zip(train_text[:5], auglist1[:5], auglist2[:5]):\n",
    "        print(\"-----Original Text: \\n\", o)\n",
    "        print(\"-----Augmented Text1: \\n\", a1)\n",
    "        print(\"-----Augmented Text2: \\n\", a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "set_global_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contextual_augment('../dataset/StackOverflow.csv', 'StackOverflow.csv', aug_p=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2, strong weak Aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eda import *\n",
    "\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "from nlpaug.util import Action\n",
    "\n",
    "def AugS(texts):\n",
    "    aug_texts = []\n",
    "\n",
    "    for text in texts:\n",
    "        aug_text = eda(\n",
    "            text,   # 原文本\n",
    "            alpha_sr=0.2,   # how much to replace each word by synonyms\n",
    "            alpha_ri=0.2,   # how much to insert new words that are synonyms\n",
    "            alpha_rs=0.2,   # how much to swap words\n",
    "            p_rd=0.2,       # how much to delete words\n",
    "            num_aug=1       # generate more data with standard augmentation\n",
    "        )\n",
    "        aug_texts.append(aug_text[0])\n",
    "\n",
    "    return aug_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AugW(texts):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    augmenter1 = naw.ContextualWordEmbsAug(\n",
    "        model_path='roberta-base', \n",
    "        action=\"substitute\", \n",
    "        aug_min=1, \n",
    "        aug_p=0.2, \n",
    "        device=device\n",
    "    )\n",
    "    aug_texts = []\n",
    "    for text in texts:\n",
    "        aug_text = augmenter1.augment(text)\n",
    "        aug_texts.append(aug_text[0])\n",
    "    \n",
    "    return aug_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('../dataset/StackOverflow.csv')\n",
    "texts = data_df['text'].tolist()\n",
    "\n",
    "aug_weak = AugW(texts)\n",
    "aug_strong = AugS(texts)\n",
    "\n",
    "data_df['w_aug'] = aug_weak\n",
    "data_df['s_aug'] = aug_strong\n",
    "\n",
    "data_df.to_csv('./StackOverflow.csv', index=False)\n",
    "print('StackOverflow, download over!!!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zfq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
